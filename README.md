# Fast Job Agent

> **Intelligent Engineering Job Aggregator** - Automated monitoring and collection of high-quality engineering positions from top tech companies

## ðŸŽ¯ Live Results

**ðŸ‘€ [Access The Ultimate AI Job Database â†’](https://www.notion.so/22aa175fa91c80c1a09fe1911721e2cc)**

### ðŸš€ The Most Comprehensive AI & Tech Job Intelligence Platform

**Want to land at the next unicorn? You need a bigger funnel.** 

Our database tracks **122 elite AI companies** across the entire ecosystem - from $80B OpenAI to seed-stage startups that could be tomorrow's giants.

#### ðŸ“Š Company Coverage Breakdown:
- ðŸ¤– **AI/ML Platforms & Foundation Models**: 28 companies (OpenAI, Anthropic, xAI, Databricks, Runway ML, ElevenLabs, Hebbia, Writer, Poolside...)
- ðŸ› ï¸ **AI-Powered Developer Tools**: 22 companies (Replit, Cursor, Stainless API, BentoML, Sourcegraph, Mintlify, OneSchema, Cribl...)
- ðŸ”’ **AI Security & Enterprise SaaS**: 15 companies (Palantir, Abnormal Security, Harvey, Kasada, Semgrep, Chainguard, FieldGuide...)
- ðŸ“ˆ **AI Data/Analytics & MLOps**: 11 companies (MongoDB, AlphaSense, Labelbox, Chalk, WorkHelix, Tonic AI...)
- ðŸ’° **AI FinTech & Trading**: 10 companies (Mercury, Addepar, Ridgeline, Imprint, Extend...)
- ðŸ¤– **AI Hardware/Robotics**: 10 companies (Anduril, Skydio, Gecko Robotics, SandboxAQ, Celestial AI, Shield AI...)
- ðŸ¥ **AI HealthTech & BioTech**: 8 companies (Abridge, Anterior, Bayesian Health, Flagship Pioneering, Tennr...)
- ðŸ¢ **Traditional Tech/SaaS**: 7 companies (SendBird, Recharge, Fleetio, Persona...)

#### ðŸ’¸ Funding Stage Distribution:
- ðŸ¦„ **Unicorns ($1B+)**: 18 companies (OpenAI, Anthropic, xAI, Databricks, Palantir, Anduril, Replit...)
- ðŸ“ˆ **Series C-D**: 15 companies  
- ðŸš€ **Series B**: 22 companies
- ðŸ’¡ **Series A**: 28 companies
- ðŸŒ± **Early Stage**: 39 companies

**Updated daily.** **Zero noise.** **Maximum AI opportunity.**

---

A high-performance async job scraping system specifically designed for Forward Deployed Engineer, Solutions Engineer, and other customer-facing engineering roles across major recruitment platforms including Ashby, Greenhouse, and Lever.

## ðŸŽ¯ Core Value

- **ðŸ’¼ Precise Targeting**: Focus on customer-facing engineering positions, filtering out irrelevant noise
- **ðŸŒ Multi-Platform Coverage**: Support for Ashby, Greenhouse, and Lever - the three major recruitment platforms
- **âš¡ High-Performance Async**: Concurrent processing, scraping hundreds of companies in seconds
- **ðŸŽ¯ Smart Filtering**: US region filtering + keyword matching for precise job targeting
- **ðŸ“Š Structured Output**: Standardized data format with CSV export and API integration support
- **ðŸ”„ Automated Monitoring**: Scheduled tasks for continuous job change monitoring

## ðŸš€ Quick Start

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Basic Usage
```bash
# Run all platform scrapers
python src/main_runner.py

# Run specific platforms individually
python src/ashby_scraper.py
python src/greenhouse_scraper.py
python src/lever_scraper.py

# View results
ls data/output/
```

### Core Functionality Demo
```python
from scraper_factory import ScraperFactory
from models import JobSource

# Create scraper instance
scraper = ScraperFactory.create_scraper(JobSource.ASHBY)

# Execute asynchronously
jobs = await scraper.scrape_all(max_concurrent=10)
print(f"Found {len(jobs)} matching positions")
```

## ðŸ“‹ Features

### ðŸŽ¯ Smart Filtering System
- **Keyword Matching**: Forward Deployed Engineer, Solutions Engineer, Customer Engineer, etc.
- **Geographic Filtering**: Automatic identification and filtering of US-based positions
- **Company Type Support**: Support for regular companies and VC portfolio batch processing

### ðŸ”§ Technical Features
- **Async Concurrency**: High-performance async processing based on aiohttp
- **Decorator Enhancement**: Automatic error handling, retry mechanisms, performance monitoring
- **Data Models**: Type-safe Job and JobStats data structures
- **Factory Pattern**: Unified scraper creation and management interface
- **Smart Configuration**: Auto-generated URLs with zero redundancy

### ðŸ“Š Data Output
- **Standardized Fields**: role_name, company_name, location, job_link, etc.
- **Multi-format Support**: CSV export, JSON API, Notion sync
- **Statistics**: Detailed scraping statistics and regional distribution

## ðŸ—ï¸ Architecture Design

### Core Components
```
â”œâ”€â”€ models.py          # Data models (Job, JobStats, ScraperConfig)
â”œâ”€â”€ base_scraper.py    # Base scraper class (common functionality)
â”œâ”€â”€ scraper_factory.py # Factory pattern (unified creation interface)
â”œâ”€â”€ utils/             # Utility modules
â”‚   â”œâ”€â”€ decorators.py  # Decorators (error handling, retry, timing)
â”‚   â”œâ”€â”€ logging_utils.py # Logging utilities
â”‚   â””â”€â”€ date_utils.py  # Date processing
â””â”€â”€ constants.py       # Constants definition
```

### Design Patterns
- **Factory Pattern**: `ScraperFactory` for unified scraper creation
- **Decorator Pattern**: `@with_error_handling` `@with_retry` `@with_timing`
- **Template Method**: `AsyncBaseScraper` defines common workflow
- **Strategy Pattern**: Platform-specific implementation strategies

### Async Architecture
```python
async def scrape_all(self, max_concurrent=5):
    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # Execute all companies concurrently
    tasks = [self.scrape_company(session, company) 
             for company in self.companies]
    
    results = await asyncio.gather(*tasks)
    return flatten(results)
```

## ðŸ“– Configuration

### Basic Configuration
```yaml
# config/ashby_companies.yaml
companies:
  - name: OpenAI
    job_board_name: openai
    # URL auto-generated: https://jobs.ashbyhq.com/openai
    
  - name: Pear VC Portfolio  
    job_board_name: pear
    is_vc_portfolio: true  # Special handling for VC portfolios
```

### Advanced Configuration
```python
# Custom keywords
FDE_KEYWORDS = [
    'forward deployed engineer',
    'solutions engineer', 
    'customer engineer',
    # ... customizable in config.py
]

# Concurrency control
scraper = ScraperFactory.create_scraper(JobSource.ASHBY)
jobs = await scraper.scrape_all(max_concurrent=10)
```

## ðŸ”Œ API Reference

### ScraperFactory
```python
# Create single scraper
scraper = ScraperFactory.create_scraper(JobSource.ASHBY)

# Create all scrapers
scrapers = ScraperFactory.create_all_scrapers()

# Get supported platforms
sources = ScraperFactory.get_available_sources()
```

### Data Models
```python
@dataclass
class Job:
    role_name: str
    company_name: str
    location: str
    job_link: str
    source: JobSource
    job_id: str
    # ... other fields
    
    def to_dict(self) -> Dict  # CSV export
    def get_unique_id(self) -> str  # Deduplication ID
```

### Decorators
```python
@with_error_handling(default_return=[])
@with_timing(log_level=logging.INFO)
@with_retry(max_attempts=3, delay=1.0)
async def scrape_company(self, session, company):
    # Automatic error handling, performance monitoring, retry mechanism
    pass
```

## ðŸ“Š Performance Metrics

- **Concurrent Processing**: Support for 10+ concurrent requests, 50+ companies scraped in 30 seconds
- **Error Recovery**: Automatic retry mechanism with 90%+ success rate
- **Memory Efficiency**: Stream processing, supports large-scale data handling
- **Geographic Filtering**: Smart US region recognition with 95%+ accuracy

## ðŸ› ï¸ Extension Guide

### Adding New Platforms
```python
# 1. Inherit from base class
class NewPlatformScraper(AsyncBaseScraper):
    async def scrape_company(self, session, company):
        # Implement platform-specific logic
        pass

# 2. Register with factory
ScraperFactory.register_scraper(JobSource.NEW_PLATFORM, NewPlatformScraper)
```

### Custom Filters
```python
def custom_filter(jobs: List[Dict]) -> List[Dict]:
    # Custom filtering logic
    return filtered_jobs

# Integrate in base_scraper.py
```

### Adding New Data Sources
```python
# 1. Add new JobSource in models.py
class JobSource(Enum):
    NEW_SOURCE = "NewSource"

# 2. Add URL template in constants.py
class URLTemplates:
    NEW_SOURCE_API = "https://api.newsource.com/{company}/jobs"
```

## ðŸ“ˆ Monitoring & Logging

### Log Levels
- **INFO**: Scraping progress and statistics
- **DEBUG**: Detailed request/response information  
- **ERROR**: Error and exception information
- **WARNING**: Configuration issues and data anomalies

### Statistics
```
ðŸ“Š OpenAI Statistics:
  Total jobs scraped: 25
  US jobs: 18
  Non-US jobs: 7
  Non-US locations: London, Toronto, Sydney
```

## ðŸ”„ Scheduled Tasks

### Cron Setup
```bash
# Run daily at 9 AM
0 9 * * * cd /path/to/fast-job-agent && python src/main_runner.py

# Run weekly on Monday with email notification
0 9 * * 1 cd /path/to/fast-job-agent && python src/main_runner.py && python src/send_report.py
```

### System Integration
```python
# Notion sync
from notion_sync import NotionSync
sync = NotionSync()
sync.sync_jobs(jobs)

# Data processing
from data_processor_pandas import JobDataProcessor  
processor = JobDataProcessor()
processed_jobs = processor.process_jobs(jobs)
```

## ðŸŽ¯ Best Practices

1. **Concurrency Control**: Adjust `max_concurrent` parameter based on target website load
2. **Error Handling**: Use decorators for automatic network exception handling and retry
3. **Configuration Management**: Use environment variables for sensitive configuration
4. **Data Validation**: Leverage dataclass for type checking
5. **Log Monitoring**: Regularly check log files and monitor scraping quality


## ðŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details